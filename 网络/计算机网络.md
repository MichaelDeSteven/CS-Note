# 计算机网络

## 概述

### 定义

计算机网络：网络是由若干个计算机和连接这些计算机的链路组成。

### 互联网的组成部分

互联网由边缘部分和核心部分组成

* 边缘部分：由所有连接在互联网上的主机组成
* 核心部分：将这些主机连接起来的路由器组成，为边缘部分提供服务

### 端系统通信方式

* 客户-服务器模式（CS结构）：一端作为客户向服务器请求，另一端作为服务器负责响应客户的请求
* 对等模式（P2P结构）：不区分客户和服务器，主机能充当客户也能作为服务器向其它主机提供服务



### 电路交换和分组交换

#### 电路交换

电路交换需要建立一条专门的物理线路，通话的两个用户始终占用端到端的通信资源，直到双方通信结束，电路交换必定是是面向连接的，需要经过建立连接—通信—释放连接这几个阶段

缺点：计算机网络具有突发性的特点，由于通信过程中不可能一直使用传输链路，因此电路交换对线路的利用率很低

优点：由于通信双方始终占用端到端的通信资源，因此电路交换保证了可靠性

#### 分组交换

报文：要发送的整块数据

分组：将较长的报文划分为一个个更小的等长数据段，在每一个数据段前面加上由必要控制信息组成的首部后，称为分组，分组是互联网中传送的数据单元

特点：采用存储转发技术，首先主机将报文划分成多个分组，路由器收到这些分组后首先会暂时存储起来，检查其首部，查找转发表，每个分组都独立地选择传输路径，待所有分组到达最终目的地时，接收端会根据分组的首部将分组还原成报文

优点

* 高速：使用存储转的分组交换，实质采用了数据通信的过程中断续分配带宽的策略，提高了通信线路的传输效率
* 迅速：以分组为传输单位，分组交换可以不用实现链接就能够向其他主机发送分组
* 灵活：路由器根据分组的目的地能够为独立地为每一个分组选择合适的转发路由
* 可靠：分组交换拥有保证可靠性的网络协议如TCP协议。

缺点：存在时延问题，例如分组在各个路由器存储转发时需要排队，造成一定的排队时延







### 计算机网络性能指标

1. **速率**：单位时间内比特的传输速率

2. **网络带宽**：单位时间内最高的传输速率

3. **吞吐量**：单位时间内网络的数据量

4. **时延**：又分为排队时延、处理时延、发送时延、传播时延

5. **时延带宽积**：传播时延*网络带宽

6. **往返时间**：在很多情况下，互联网上的信息往往是双向交互，因此需要知道它们的往返时间

7. **利用率**：分为信道利用率和网络利用率，利用率和时延是一对矛盾关系，利用率越高，时延也就越高



### 计算机网络体系结构

计算机网络采用分层的好处

1. 各层之间相互独立，将复杂的计算机网络分解成若干个易处理的问题，降低问题的复杂度，每一层不需要知道下一层如何实现，而是直接调用该层的接口服务
2. 灵活性好：当某一层需要发生变化时，接口关系不变，那么直接或者间接调用当前层的接口将不受影响
3. 易于实现和维护：分层设计使得计算机网络这样一个庞大而复杂的系统变得易于实现和调试

一般有三种计算机网络分层：OSI七层模型、5层协议以及TCP/IP的4层体系

#### TCP/IP

TCP/IP由应用层、传输层、网络层、网络接口层组成

#### 5层协议

由应用层、传输层、网络层、数据链路层、物理层

#### OSI七层模型

OSI七层模型自底向上有：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层

* 物理层：考虑的是怎么样在传输媒体上传输数据比特流，而不是具体的传输媒体。物理层的作用是尽可能屏蔽通信手段和传输媒体的差异，使数据链路层感受不到这些差异
* 数据链路层：为了解决两台主机的数据传输。发送时报文是一段一段的链路上传输的，这就需要专门的链路层协议。将网络层传下来的IP数据包封装成帧，每一帧都包含了数据和必要的控制信息（同步信息、地址信息、差错控制），接收数据时，还需要直到数据部分的起始和结束的比特，才能提取出数据上交给网络层
* 网络层：主要负责为不同主机提供通信服务，使用了IP地址来标识不同的主机，确定数据包传输路径和负责将数据包转发到目的地
* 传输层：负责向两台主机的进程之间的通信提供通用的服务，主要使用的是TCP/UDP协议
* 会话层：在不同的机器上的用户建立以及管理会话
* 表示层：数据的压缩、加密解密
* 应用层：为特定的应用程序提供传输服务



![](https://raw.githubusercontent.com/MichaelDeSteven/CS-Note/master/%E7%BD%91%E7%BB%9C/pic/osi.png)





## 物理层

主要任务：将数据链路层上的帧转换成比特流，然后在传输媒体上传输，同时尽可能屏蔽来自传输媒体或者传输手段的差异



### 网络设备

#### 集线器

集线器位于物理层，它是一个具有多个端口的设备，主要功能是对接收的信号进行再生整形和放大，集线器不智能不会过滤数据，唯一的作用是将某一端口接收到的比特流复制到它所有端口，连接到该集线器的所有设备都可以收到数据。这种做法会产生安全问题，同时也会对网络造成不必要的带宽浪费

#### 网桥

多个集线器连接在了一起，又是集线器时广播通信，所以我们需要一种设备，能够有效的隔离子网。网桥的出现能够使得局域网得到有效的划分

网桥是位于数据链路层的，其工作原理是它内部有含有一张过滤表，网桥工作一段时间后会记录所有出口的MAC地址，当网桥收到数据帧检查目的MAC地址，然后再确定将该帧转发到哪一个接口，或把它丢弃



#### 交换机

交换机工作在数据链路层，交换机与集线器很相似，也是一个具有多个端口的设备，但是它与集线器不同的地方在于它是“智能”的，它拥有自学习的功能。同时它也是一个多接口网桥，网桥只能一次分析和转发一个帧，而交换机具有并行性，同时连通多对接口，使多对主机能同时通信，通信过程中是独占传输媒体、无碰撞地传输数据。

自学习过程

![](https://raw.githubusercontent.com/MichaelDeSteven/CS-Note/master/%E7%BD%91%E7%BB%9C/pic/自学习.png)



#### 路由器

路由器工作在网络层，当它收到一个数据包时会检查数据包的IP地址，IP地址不合法则直接丢弃，若IP地址属于自己网络就接受它，如果不是发送给自己所在网络的将会查找路由表将该数据包转发到目的路由



#### 网卡

位于数据链路层，该硬件标识了硬件地址又称为物理地址或MAC地址，由48位构成



## 数据链路层

* 链路：指的是相邻结点传输数据的物理路线
* 数据链路：链路加上一些必要的通信协议来控制数据的传输

### 基本问题

* 封装成帧：在数据包的前后加上首部和尾部，然后就构成了帧，在帧首部和尾部进行帧定界，来确定帧的开始和结束位置
* 透明传输：解决数据部分被当作是帧的结尾从而影响数据的传输的问题，可以采用字节填充的方式或者加入转义字符
* 差错检测：比特传输过程中，有可能出现比特差错：0变1，1变0，数据链路层需要有差错检测的措施，一般采用CRC冗余校验



### 点对点信道

#### ppp协议

**构成**

![](https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=353916427,4220333459&fm=26&gp=0.jpg?ynotemdtimestamp=1610158026833)



* F（flag）字段表示PPP协议的边界，值为0X7E
* A（address）字段表示地址，由于是点到点的，所以值始终为FF
* C（Controll）：控制字段，0x03目前没有特殊意义
* 协议字段：LCP（链路控制协议）
* FCD：帧校验序列，通过循环冗余



**字节填充**

* 信息字段出现0x7E则转化为（0x7D, 0x5E）
* 信息字段出现0x7D则转化为（0x7D,0x5D）
* 信息字段出现小于0x20的字符，则在该字符前加入一个0x7D，同时将该字符的编码加以改变，比如0x03会转化为(0x7D,0x23)



**零比特填充**

* 发送端扫描数据字段，出现连续5个1则立即填入1个0
* 接收端扫描数据字段，出现连续5个1则把后面的0删除



**CRC循环冗余校验**

* 发送端将数据分组，假定每组k个比特，CRC运算就是将待传送数据添加待检测的冗余码，然后构成一个帧发送出去
* 假定冗余码为n位，那么就在带传送数据后面加入n位0得到M，双方协商一个除数P，然后M除以P得到余数R即为冗余码，将冗余码添加到M即为发送的数据
* 接收端将接收的数据以帧为单位进行CRC检验：把每一个帧都除以同一个除数P，检查余数R是否为0，如果R不为0那么这个帧出现了差错，将其丢弃





### 广播信道

* 局域网：地理范围和主机数量有限，网络为一个单位所有
* 以太网：实现以太网的协议

**MAC帧格式**

![](https://raw.githubusercontent.com/MichaelDeSteven/CS-Note/master/%E7%BD%91%E7%BB%9C/pic/MAC帧格式.png)

* 以太网最大传输单元MTU=1500
* MAC帧最小为64字节，目的地址+源地址+类型+FSC一共占18个字节
* 类型只有两种IP协议（0x0800）和ARP协议（0x0806）



#### CSMA/CD协议

* 载波监听：在发送前和发送后不断检测信道，发送前检测是为了获取发送权，发送后检测是为了及时检测发送的数据有没有与其它站碰撞
* 多点接入：说明了该网络是一个总线型网络，多个用户接入一根总线
* 碰撞检测：便发送变监听信道，当发送数据与其他站产生冲突时，则停止发送数据等待一段随机时间再发送



MTU是什么？为什么被设定为1500字节

MTU指数据链路层传输包的最大传输单元，1500字节是综合考虑了信道利用率和信道状态的折中方法，若帧长过短那么信道利用率低，而帧长过长，出错重传效率会降低。

1. 数据包越大，出错的可能性越大
2. 数据包一旦重传，重传的花费会变多，进而导致网速变慢
3. 时延增大，数据包在被完整收到的时候才会开始校验检查，确认收到的每个bit都没有出错，如果帧过大，将花费过多的时间在校验上，就无法及时将包交给上层
4. 最初以太网工作方式是采用载波监听多点接入/碰撞检测CSMA/CD，所有机器是半双工通信，因为网络是共享的，不能让机器占用太长时间网络，这样会影响其他机器的通信，同时MTU也不能过短，这和以太网帧半双工通信有关，比如在10Mbs以太网当中，争用期2τ = 51.2μs（最迟可以确定没有发生碰撞的时间）可以发送512比特即64字节，假定某个站发送了一个很短的帧，在发送完毕之前没有检测出碰撞。假定这个帧在继续向前传播到达目的站时发送了碰撞，那么目的站会收到有差错的帧，发送站却不知道这个帧发生了碰撞。





## 网络层

任务：将各种异构的网络连接起来，提供主机之间通信的服务

### 网络层提供的服务

网络层应该提供什么样的服务？为什么？

网络层提供**简单灵活、面向无连接、尽最大努力交付**的服务，即可靠交付交给端系统来负责，因为这样做的好处使得网络造价大大降低，运行方式灵活，能适应多种应用。

| 对比方面                       | 虚电路服务                                 | 数据报服务                                       |
| ------------------------------ | ------------------------------------------ | ------------------------------------------------ |
| 思路                           | 可靠通信应当由网络来保证                   | 可靠通信应当由端系统来保证                       |
| 连接的建立                     | 面向连接                                   | 面向无连接                                       |
| 终点地址                       | 仅在连接阶段使用，每个分组使用短的虚电路号 | 每个分组都有终点的完整地址                       |
| 分组的转发                     | 属于同一虚电路的分组按照同一路由进行转发   | 每个分组独立选择路由进行转发                     |
| 结点出现故障时                 | 所有通过故障结点的虚电路均不能工作         | 出故障的结点可能会丢失分组，一些路由可能发生变化 |
| 分组的顺序                     | 按顺序到达终点                             | 到达终点的时间不一定按照顺序                     |
| 端到端之间的差错处理和流量控制 | 由网络负责                                 | 由主机负责                                       |







### 网际协议IP

#### **IP数据报格式**

一个IP数据报由首部和数据组成，首部前一部分固定长度为20字节

![](https://img-blog.csdn.net/20131220194104812?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2UxMjNfemhvdXdlaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

1. 版本：占4位，标识IP协议的版本号，如目前广泛使用的IP协议版本号为4（即IPv4）、起步阶段的IPv6
2. 首部长度：占4位，值为1表示4个字节，由于首部最短长度为20个字节，所以值最小为5，若可选字段不为4字节的整数倍，就会用填充字段来填充尾部
3. 区分服务：8位，用来获得更好的服务，一般情况不用
4. 总长度：指首部和数据之和的长度。单位为字节，16位表示数据报字段最大为$2^{16}-1$
5. 标识：占16位，计数器，用来产生IP数据报的标识，由于数据报超过MTU会被分片，标识字段保证相同标识号的片段能重新组成原来的包
6. 标志：占3位，目前只有后两位有意义，标志字段中间一位是DF（Dont Fragment）0表示无分片，末位是MF（More Fragment）MF=1表示后面有分片
7. 片偏移：占13位，较长分组分片后，根据片偏移和标识字段来恢复原分组，片偏移单位为8字节
8. 生存时间：占8位，简称TTL（Time to Live）表明数据包的寿命，为了防止数据报无限制地在互联网兜圈子，以路由器跳数为单位，最大值为255，TTL为0则丢弃数据报
9. 协议：占8位，指明携带的数据使用了何种协议，如TCP、UDP协议，使得目的主机IP层知道应将数据部分上交给哪个协议处理（TCP为0x06，ICMP为0x01）
10. 首部检验和：只检验数据报的首部，路由器每次收到数据报都要重新计算检验和，不检验数据部分可以减少计算工作量
11. 源地址：32位
12. 目的地址：32位
13. 可选字段：用来支持排除、测量和安全，可选字段长度可变，最终会被填充成为4字节的整数倍
14. 数据部分：数据包携带的信息



#### IP地址

介绍：IP地址就是给每个连接在互联网上的主机（或者路由器）分配一个全世界范围内的唯一的32位标识符

特点

* IP地址是分等级的：首先是便于管理，IP地址管理机构在分配IP地址时只分配网络号，而剩下的主机号由得到的网络号单位自行分配。其次是路由器根据网络号来转发分组，大大减少了路由器表项的条目数，从而减少了路由表的占用内存。
* IP地址是标识主机或者路由器的接口
* 用转发器或者网桥连接起来的若干个局域网仍为一个网络
* IP地址是平等的



**编制方式**

1. **二级地址**

IP地址:={<网络号>, <主机号>}

根据网络号来划分A、B、C、D、E类地址

![](https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=2428428384,3350566780&fm=26&gp=0.jpg?ynotemdtimestamp=1610608168894)



记忆技巧

* 从0-255划分为一半，取右半段继续划分直到总段数为5段则划分完毕

```
 A：0xxxxxxxx  私有地址：10.0.0.0-10.255.255.255
 B：10xxxxxx 私有地址：172.16.0.0-172.16.255.255
 C：110xxxxx 私有地址：192.168.0.0-192.168.255.255
 D：1110xxxx
 E：1111xxxx
```



2. 划分子网

解决的问题：解决了IP地址利用率低的问题，假如一个网段申请了C类地址，该网络可以接入255台主机，但是却用不了那么多，因此利用子网划分的技术可以将该网段分成两个子网。

IP地址:={<网络号>,<子网号>,<主机号>}

子网掩码：由连续的1组成用来判断网络号和子网



3. 构造超网

解决的问题：通过路由聚合将相同网络前缀的网络号合并在了一起，减少了路由表项数目的大小

IP地址:={<网络前缀号>, <主机号>}

无分类编址CIDR消除了传统的A、B和C类地址以及子网的概念，使用网络前缀和主机号来进行编码，网络前缀的长度可以根据需要变化。

路由表查找算法：最长前缀匹配，使用了字典树的底层数据结构



#### 特殊IP地址

0.0.0.0：表示本网络的本地

255.255.255.255：广播地址，表示该网络

127.0.0.1：网络号为127.X.X.X被称为环回地址，常用于检测本机TCP/IP协议工作是否正常，localhost主机名与此具有相同意义





#### IP地址与MAC地址

1. MAC地址是硬件实现的，也叫做物理地址，一般是无法改变的，而IP地址是逻辑实现的，通过分配或者自定
2. 物理地址是物理层、数据链路层使用的，而IP地址是网络层以上使用的地址
3. IP地址是指向最终目的地的，而数据包每经过下一跳路由器，其物理地址都会发生改变

使用IP地址通信而不使用MAC地址通信的原因：世界上的网络大多都是异构的，各个网络进行通信必须进行非常复杂的硬件地址转换，而使用通过ARP地址解析协议，通过虚拟的IP，降低了硬件地址转换的复杂度，同时用户视图上屏蔽了网络层下面复杂的过程



#### 分组转发算法

* 普通的路由转发算法

1. 根据数据包获取目的IP地址D进而网络地址N
2. 判断是否为本地，若网络N与此路由器直接相连，则直接将数据报交付给目的主机D，否则是间接交付，执行3
3. 特定主机路由：若路由表中的目的地址为D的特定主机路由，则把数据报传送给路由表中所指明的下一跳路由，否则执行4
4. 下一跳路由：若路由表有到达网络地址N的路由，则把数据报传送给路由表所指明的下一跳路由（由路由器找到下一跳路由IP地址，然后由网络接口软件使用ARP将IP地址转化为硬件地址，并将此物理地址放在链路层的MAC帧的首部，然后根据该物理地址找到下一条路由）
5. 默认路由：若路由表中有默认路由，则把数据报传送给路由表中所指明的默认路由器；否则执行6
6. 分组出错：报告转发分组出错



#### 虚拟专用网VPN

作用：使外部主机访问专用网。





#### 网络地址转换NAT

作用：专用网内部主机使用本地IP地址想访问互联网，需要把将本地IP转换为全球IP



### 地址解析协议ARP

作用：已知一个主机或路由器的IP地址，需要找到其MAC地址

过程

1. 若ARP高速缓存表中有指定IP地址到物理地址的映射则找到结果，否则执行2
2. ARP进程在本局域网上广播一个ARP请求分组
3. 本局域网内的所有主机上运行ARP进程都收到此ARP请求分组
4. 某主机与ARP请求分组要查询的IP地址一致，就收下这个ARP请求分组（当前主机ARP表也会保存发起者的IP地址到硬件地址的映射），并向发出者发送ARP响应分组（单播），这个响应分组携带了当前主机的物理地址
5. 发出者收到响应分组后，将目标IP地址对应的物理地址存入ARP高速缓存表



### 网际控制报文协议ICMP

作用：网络包在网络的传输是复杂的，ICMP用于告知网络报文传输的差错和控制信息，来更加有效地转发IP数据报和提高交付成功机会



#### ICMP询问报文

1. 回送请求（8）和回答（0）报文：主机或路由器向特定的目的主机发送回送请求报文，收到该报文的主机必须向源地址发送会送回答报文，用于了解目的主机是否可达和状态
2. 时间戳请求和回答报文：用于请求目的路由器或者主机回答当前的日期和时间，用于时钟同步和测量时间





#### ICMP差错报文

1. 终点不可达（3）：当路由器或主机不能交付数据报时就向源点发送终点不可达差错报告报文
2. 时间超过（11）：当路由器收到生存时间为0的数据报，丢弃该数据报后，再向源点发送时间超过差错报告报文
3. 参数问题（12）：当数据报首部字段的值不正确时，就丢弃该数据报，并向源点发送参数问题请求报文
4. 改变路由（重定向）（5）：路由器把改变路由报文发送给主机，让主机知道下次应将数据报发送到另外的路由



#### 应用

* PING命令：测试两台主机的连通性
* Traceroute命令：用于跟踪路由或者确定路径上路由的MTU





**PING工作原理**

![](C:\Users\Steven\Desktop\CS-Note\网络\pic\回送请求报文字段.png)

PING的报文段解析

* 类型字段：标识该报文是回送应答（0）还是回送请求（8）报文
* 代码字段：为0
* 检验和字段
* 标识符字段：应用程序的PID，标识该报文是由哪个应用程序发送的
* 序号字段：发送一次新的回送请求就加一





**traceroute**

traceroute充分利用了ICMP差错报文，它主要有两个作用

* 设置特定的TTL，用来跟踪路由
* 故意设置禁止分片，用来确定路径上的MTU







### 路由器结构

* 由多个输入输出端口组成的专用计算机，其任务是转发分组

路由器结构可以分为两个部分，路由选择部分和分组转发部分

![](https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=1498252813,1934406112&fm=26&gp=0.jpg)



**路由选择部分**

* 根据路由选择协议构造出路由器表，同时经常或者定期和相邻的路由器交换路由信息从而不断更新和维护路由表





**分组转发部分**

* 由交换结构、输入输出端口，作用是根据交换结构中的转发表对分组进行处理，将某个输入端口进入的分组从一个合适的输出端口转发出去。



### 路由选择

理想的路由算法：正确和完整、稳定、计算简单、公平、最佳

最佳的路由算法：相对于某种特定条件下较为合理的路由算法

静态路由选择协议：简单、开销小，不能适应网络状态的变化

动态路由选择协议：自适应路由选择，能较好地适应网络状态的变化，实现较为复杂、开销也较为大

自治系统AS：互联网使用分层次的路由选择协议，因此可以把互联网划分为许多个较小的自治系统即AS，AS使用了单一的技术来管理AS内的所有路由器，这些路由器选择同一路由选择协议和共同度量来确定分组在AS内的路由，同时还使用一种AS之间的路由选择协议确定分组在AS之间的路由

内部网关协议：自治系统内选择的路由选择协议

外部网关协议：自治系统之间选择的路由选择协议



#### 内部网关协议IGP

**RIP**

基于距离向量的路由选择协议，最大的优点是简单

* 和相邻的路由器交换路由信息
* 信息包含了某网络、到达某网络所需要的跳数，下一跳的路由器
* 按固定时间间隔交换路由信息

距离向量算法

对每个相邻的路由器发送的RIP报文执行以下步骤

1. 将下一跳路由改为当前路由器，跳数均增加1
2. 检测所有条目，若原来路由表中没有目的网络N，则把该项目添加到路由表，否则执行3
3. 若下一跳路由器与原来路由表中的条目相同则直接更新（以最新的消息为准），否则执行4
4. 下一跳路由器不同，比较跳数大小，若跳数比原来小则更新（距离变小，应该更新）否则执行5
5. 什么都不做

评价

1. 实现简单开销小，适用于小型的网络，最大跳数为16（表示不可达）
2. 好消息传得快，坏消息传得慢（当网络故障时，要经过较长时间才能将此信息传送给所有的路由器）



**OSPF**

基于Dijkstra算法的一种链路状态的路由选择协议，具有更新过程收敛快的特点

* 采用泛洪法向自治系统内的所有路由器发送信息
* 信息为相邻的所有路由器的链路状态（与哪些路由器相邻，到相邻路由器的代价）
* 只有当链路状态发生变化时，才向所有路由器用泛洪法发送信息

链路状态数据库：各路由器频繁交换信息后，所有路由器最终都会建立一个链路状态数据库，也即全网的拓扑结构图

区域划分

原因：为了更好地管理较大的网络，将泛洪法交换链路信息的范围缩小到某个区域，从而减小整个网络的通信量

分层次的区域划分

主干区域：位于上层的区域，标识符为0.0.0.0，用于连通下层区域



分组类型

* 问候分组：用来发现了和维持邻站的可达性
* 数据库描述分组：向邻站给出自己的链路状态数据库中的所有链路状态的摘要信息
* 链路状态请求分组：向对方请求某链路状态的项目的详细信息
* 链路状态更新分组：用泛洪法对全网更新链路状态的信息
* 链路状态确认分组：对链路状态更新分组的确认

OSPF基本操作

* 问候分组确认可达性

每两个相邻路由器每隔10s要交换一次问候分组。确认邻站是否可达

* 其它分组类型用于同步链路状态数据库

路由器并不是将自己的链路状态信息向全网广播的，虽然这样能得到完整的链路状态数据库，但是开销太大，OSPF是这样做的：让相邻的路由器用数据库描述分组用来交换自己的链路状态摘要信息，交换完数据库描述分组后，若当前的数据库发现缺少了某些链路状态的详细信息，就会发起链路状态请求分组来进行同步

* 路由器链路状态发生变化

当一个路由器链路状态发生变化，该路由器就使用链路状态更新分组，用泛洪法向全网更新链路状态。这种泛洪法是**可靠**的，所谓可靠是指收到更新分组的路由器会发送确认分组

其它特点

1. 灵活：对不同的链路设置不同的代价
2. 负载平衡
3. 支持可变长度的子网划分和无分类编制的CIDR
4. 所有OSPF路由器之间交换的分组都具有鉴别的功能，保证了仅在信赖的路由器交换链路状态信息





#### 外部网关协议BGP

特点

1. 每个自治系统最少选择一个路由器作为发言人
2. 使用TCP连接，再建立BGP会话交换路由信息





### IPv6

![](C:\Users\Steven\Desktop\CS-Note\网络\pic\ipv6.png)



**IPv6对IPv4有以下几个改进**

* 地址由32位扩充到了128位，解决了公有IP地址严重不足的问题
* 取消了首部校验和。因为数据链路层与传输层都会进行校验
* 取消了IP分片与重组相关字段。分片与重组是耗时的。取消后将大大提高路由转发速度





## 传输层

### 概念

两台主机之间的通信实质就是两台主机的应用进程相互通信，而传输层就是负责应用进程之间的**逻辑通信**，提供多路的复用和分用

#### 端口

定义：由于各个操作系统指派进程标识符可能格式不同，而且进程的创建和销毁是动态的，因此采用协议端口号的方式来标识应用进程，端口号由16位标识又分为熟知端口号（0-1023）和短暂端口号

复用：将多个应用进程数据通过传输层传送到网络层

分用：将网络层收到的数据分发给各个应用进程

![](https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=3215791253,4029579005&fm=26&gp=0.jpg)

#### TCP与UDP的比较

UDP只在IP数据报之上增加了很少的一点功能：复用和分用功能，差错检测功能

* 可靠性：TCP是可靠传输，UDP是尽最大努力交付
* 连接：TCP是面向连接传输，UDP无连接
* 拥塞控制、流量控制：TCP有拥塞控制、流量控制，UDP则没有
* 传输方式：UDP是面向报文传输，而TCP是面向字节流传输
* 服务对象：UDP支持多播和广播，而TCP仅能一对一通信
* 首部开销：UDP首部开销小只有8个字节，而TCP的首部固定长度就有20字节
* 分片不同：TCP的数据大小如果大于MSS则在传输层分片，UDP数据如果大于MTU则会在IP层分片

应用场景：需要可靠、准确的传输就使用TCP比如FTP、HTTP，不要求太准确但要求快可以使用UDP，如DNS，广播视频等





### UDP协议

#### 格式

![](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fs3.sinaimg.cn%2Fmiddle%2Fa7f4cc614c325b22f9c72%26690&refer=http%3A%2F%2Fs3.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1613303701&t=36dfd93b1e8232aea58c6004131580e9)

* 源端口号：2个字节
* 目的端口号：2个字节
* 用户数据报长度：2个字节，最小值为8
* 检验和：2个字节

#### 特点

* 面向无连接
* 尽最大努力交付
* 没有拥塞控制
* 支持一对一、一对多、多对一、多对多的交互通信
* 面向报文
* 首部开销小，长度只有8字节



### TCP基本

#### 什么是TCP

TCP是可靠的、面向报文的、基于字节流的传输层协议。



#### 什么是TCP连接

TCP连接是逻辑上的连接。通信双方会维护当前连接的状态信息（如Socket、序列号、窗口大小等），以保证连接可靠性



#### TCP如何保证可靠传输

* 校验和：检测TCP的头部和数据是否存在比特差错，如果存在TCP直接将其丢弃
* 序列号：接收端根据接收的序列号判断是否见过该分组，存在则丢弃
* 确认号：接收端发送序列号+1作为确认号的分组给发送端，表示确认收到了该分组
* 超时重传：根据重传计时器的时间来判断分组是否丢失，丢失则重传
* 滑动窗口：记录已经收到的分组以及需要发送的分组，依据分组的状态来发送
* 流量控制：使得接收端能来得及接收发送端的数据
* 拥塞控制：保证在拥塞的网络能够较好的接收分组



#### 如何标识一个TCP连接

TCP四元组可以唯一确定一个连接，四元组包括了源地址、源端口、目的地址、目的端口



#### 格式

![](https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=3436320348,126020379&fm=26&gp=0.jpg?ynotemdtimestamp=1610783150747)

* 源端口号：16位
* 目的端口号：16位
* 序号字段：32位，TCP连接是面向字节流的，传送的数据流中每个字节都会编上一个序号，这个序号表示这个数据包所发送的数据的第一个字节的序号，每发送一次数据，就累加依次该数据字节的大小。**用来解决网络包乱序问题**
* 确认字段：32位，期望收到下一个报文段的序号，表明该序号之前的网络包都已经被正常接收。**用来解决不丢包问题**
* 数据偏移：4位，指出TCP报文数据起始距离TCP报文段的起始处有多远（即TCP首部长度）
* 保留字段：6位，保留为今后使用
* 紧急比特URG：URG=1表明紧急指针字段有效。它告诉系统此报文段有紧急数据，应尽快传送
* 确认比特ACK：ACK=1表明确认字段有效，TCP规定除了最初建立连接时的SYN包之外改为必须设置为1
* 推送比特PSH：PSH=1的报文段，会尽快交给应用程序，而不再等到整个缓存都填满后再向上交付
* 复位比特RST：当RST=1表明当前TCP连接出现严重差错，应释放连接，然后再重新建立连接
* 同步比特SYN：SYN=1表明这是一个连接请求或者连接接受的报文
* 终止比特FIN：FIN=1表示此报文段的发送方数据已经发送完毕，请求释放TCP连接
* 窗口字段：16位，用来让对方设置发送窗口的依据，明确指出了现在允许对方发送的数据量（例如：发送一个报文段确认号为701，窗口字段为1000时，表示发送方的接收缓存空间还可以接收1000个字节）
* 校验和字段：16位，校验包括首部和数据部分
* 紧急指针字段：16位，指出本报文段中紧急数据有多少字节（紧急数据放在了报文段数据的最前面）
* 选项字段：长度可变
* 填充字段：使报文的首部长度为4字节的整数倍



注：TCP/IP详解的保留字段为4位，新增两个字段

* 拥塞窗口减比特CWR：发送方降低它的发送速率
* ECN回显比特ECE：发送方接收到了一个更早的拥塞通告



#### MSS和MTU

**IP层既然能够分片，为什么TCP层还需要MSS？**

* MTU：最大传输单元，即一个网络包的最大长度，一般最大为1500字节
* MSS：最大报文段，即出去IP和TCP头部后的TCP数据



A：为了提高传输效率。因为IP层是尽最大努力交付而不是可靠交付，那么如果一个IP分片丢失，整个IP分片将会重传。如果是TCP分片，那么接收方如果发现某一TCP报文丢失，则只会重发这个TCP报文，提高了传输的效率。



#### Linxu查看TCP状态

```shell
netstat -napt
```



![](C:\Users\Steven\Desktop\CS-Note\网络\pic\netstat.png)







### 三次握手与四次挥手



#### 三次握手过程

![](https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=2930509917,3519585697&fm=26&gp=0.jpg)

服务端处于监听状态

* 第一次握手：客户端随机选择一个起始序号x并将其置于SYN序列号字段，然后将syn标志位设置为1表示该SYN报文，将该报文发送给服务端，客户端进入SYN_SENT状态
* 第二次握手：服务端（一般为被动接收方）收到请求后，服务器也随机选择一个起始序号y将其置于序列号中，然后将确认号字段置为x+1，将SYN和ACK标志位置为1，将SYN+ACK报文发送给客户端，服务器进入了SYN_RECV状态
* 第三次握手：客户端收到了服务器的连接响应报文，客户端会最后发送一个回应报文给服务器，该报文段首先将ACK标志位置为1，确认字段置为y+1，最后把报文发送给服务端，该报文可以携带数据，发送完成后，客户端进入ESTABLISHED状态，服务端接收成功后，双方都进入ESTABLISHED状态







#### 三次握手的原因

* 阻止重复历史连接的初始化：网络拥塞情况下，会连续发送多个SYN报文建立连接，三次握手中，如果旧的SYN报文比新的SYN早到达服务端的话，服务端会回复SYN+ACK给客户端，而客户端会根据上下文判断这是一个历史连接，此时将会发送RST包来阻止历史连接的建立。如果是两次握手的话，那么在服务端发送完SYN+ACK报文后，历史连接将被初始化
* 同步双方的序列号：序列号可用于标识发送的数据包哪些被对方收到，接收方也能根据序列号按序接收数据包。如果是两次握手，只保证了一方的初始化序列号能被对方接收到，因而没办法保证双方的初始化序列号都能被确认接收。
* 避免资源的浪费：两次握手下，如果连续发送多个SYN报文，将可能建立多个历史连接，造成资源的浪费





#### ISN的生成

为什么客户端和服务端的ISN不相同？

* 防止旧的连接的报文被新的连接接收，导致数据错乱
* 安全性，防止黑客伪造相同序列号的TCP报文被对方接收



ISN随机生成算法

* ISN = M + F（四元组）
* M表示一个计时器，每隔4ms加1
* F表示Hash算法，根据四元组随机生成一个值



#### SYN 超时处理

发送方发送syn包，得不到syn+ack包的回复就会一直重试，在Linux中默认5次，重传的时间采用的是二进制退避算法，即一开始为1s，每次重传时间加倍，发送五次得不到结果就会断开连接





#### SYN攻击



**原理**

一个或多个恶意客户端伪造源IP地址，向服务器发送一系列SYN报文段，服务器收到SYN报文段，会给每一个连接创建分配一定数量的连接资源。由于连接尚未建立，服务器会因为维护大量的半打开连接耗尽自己的内存后，从而拒绝为后面的合法连接服务



**防范**

* 法1增大半连接队列大小：修改Linux内核参数，控制队列大小，队列超出大小时丢弃处理。
* 法2开启syncookie功能：当SYN队列满之后，后续的SYN报文到达后不进入SYN，而是将计算一个cookie值放入SYN+ACK的序列号字段并返回给客户端，只有当SYN+ACK报文段被确认后且应答报文的ACK合法时，才将该队列放入Accept队列中
* 法3减少SYN+ACK次数







![](https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=1797973876,956426254&fm=26&gp=0.jpg)



#### 四次挥手过程

由于TCP连接是全双工的，因此每个方向都必须单独关闭。当某一方完成完成它的数据发送任务后就能发送一个FIN来终结这个方向的连接。

* 第一次挥手：客户向主机发送FIN报文段（FIN=1，seq=u），表示关闭当前方向的数据传送，并进入FIN-WAIT-1状态，表示没有数据要传送了。
* 第二次挥手：服务器收到FIN报文段后，向客户发送确认包（ACK=1，seq=v，ack=u+1），然后进入COLSE-WAIT状态，这时候客户进入了FIN-WAIT-2，即处于半关闭状态，若服务器还有数据要传输给客户，客户端还需要接收。
* 第三次挥手：等到服务端给客户端发送完数据后，就向客户发送FIN报文段（FIN=1,ACK=1，seq=w，ack=u+1），服务端进入LAST-ACK状态。
* 第四次挥手：客户端收到FIN报文段后，向服务器发送，回复确认报文（ACK=1,seq=u+1，ack=w+1）给服务端，服务端收到确认包后直接进入COLSED状态。若经过2MSL时间，没有来自服务端的FIN报文段后，客户端进入COLSED状态。



CLOSE-WAIT状态：含义是等待关闭，当对方发送连接释放请求报文，系统回应一个ACK报文给对方，则进入了CLOSE-WAIT状态，这个状态是为了确认是否还有数据要发给对方，若没有那么系统就可以发送一个FIN报文来关闭连接



#### TIME_WAIT详解

MSL：最大报文段生存时间，指任何报文段在网络上存在的最长时间，超过这个时间报文将被丢弃

**什么是TIME-WAIT？**

TIME-WAIT状态是指TCP主动关闭方收到被对方的中止连接报文并发送应答包后进入的状态，如果在2MSL时间后未收到对方的FIN报文则直接进入CLOSED状态。

**为什么要TIME-WAIT？**

* 保证全双工通信的连接正常关闭，TIME-WAIT状态下，主动关闭方将发送ACK报文，确保能被被动方能接受，从而帮助其正常关闭。
* 防止旧的连接报文在新的连接中出现（通过2MSL保证了某个方向和另一个方向的旧的连接报文都能够网络中自然消失）



**等待时间是为什么是2MSL？**

比较合理的解释是，当主动关闭方最后发送确认报文时，TIME-WAIT计时器开始计时，如果报文在MSL后未到被动关闭方，被动关闭关就会再次发送FIN报文，一来一去正好2个MSL，如果未收到，主动关闭方则认为被动方已经接收到了应答报文，因此直接进入CLOSE状态

如果**过短**：主动关闭方先进入CLOSE状态，被动关闭方还在LAST ACK状态，如果主动方发起建立连接的SYN报文后，被动关闭方将会发送RST报文。



**TIME-WAIT过多危害？**

* 连接会占用内存资源
* 占用端口资源，如果端口被消耗完毕，将会导致无法创建新连接



**优化TIME-WAIT**

* 打开tcp_tw_reuse和tcp_timestamps，复用旧的连接



#### COLSE_WAIT详解

出现大量CLOSE_WAIT状态的原因及解决方法

一般发生于服务端，服务端与客户端通信时，因服务器未关闭socket，而导致COLSE_WAIT过多，会导致服务端打开大量fd，最终导致达到最大fd数量上限，导致无法处理新的socket请求





#### KeepAlive

KeepAlive的**必要性**

* 许多防火墙等会自动关闭空闲的 socket 连接，TCP KeepAlive可以防止某些短时间的空闲 socket 不会被关闭；
* 对于一些非正常断开的连接，如断电，服务器端并不能检测到，为了回收资源，必须提供一种检测机制；

KeepAlive的两种方式

* TCP 协议自带的 KeepAlive
* 应用层面的心跳包



**TCP 中已有 SO_KEEPALIVE 选项，为什么还要在应用层加入心跳包机制？** 
主要是因为 TCP 协议中的 SO_KEEPALIVE 有几个致命的缺陷：

* keepalive 只能检测连接是否存活，不能检测连接是否可用。比如服务器因为负载过高导致无法
  响应请求但是双方的连接仍然存在，此时 keepalive 无法判断该连接是否可用。
* 不及时。如果 TCP 连接中的另一方因为停电突然断网等非正常断开的现象，由于服务器端（被动连接/断
  开的一方）并不知道客户端已断开连接，此时若服务器正在发送数据，那么会导致数据发送失败并进行
  数据重传，由于重传包的优先级要高于 keepalive 的数据包，因此 keepalive 的数据包无法及时发送出
  去。
* 浪费系统资源。当重传超过一定次数，TCP 协议会发送 keepalive 探测包到客户端，一旦探测包没有返回，服务
  器端会以 keepaliveinterval 的频率继续发送探测包，经过若干次重试，若服务器一直没有收到应答就
  会认为该 TCP 连接已经断开（默认时长是 2 小时），而 2 小时以内这个连接一直不会断开，浪费系统资
  源。







#### 半打开和半关闭

**半打开**

TCP连接后，如果未告知另一端情况下通信的一端关闭或者终止（比如切断电源），这样另一端仍然会认为这条链接是有效的

* 异常关闭的一端再次收到另一端的数据时，对此连接并不知情，因此会向另一端发送重置报文段进行响应，两端的连接也因此会被关闭
* 使用KeepAlive机制也能保证该连接正常关闭



**半关闭**

TCP连接为全双工，客户端到服务端和服务端到客户端是两条传输方向，半关闭应用于这样一个场景，A向B方向的数据传输完毕，但是A仍希望接收B方的数据，因此可以采用半关闭的方式





#### Socket编程



**函数相关**



* bind：服务端绑定IP地址和端口
* listen：服务端监听
* accpet：进入阻塞，等待客户端连接，如果有连接进来会返回一个已完成连接的Socket用来传输数据
* connect：向服务端发起请求
* read/write：读写
* close：关闭连接





**半连接队列**

* 客户端发送SYN包，服务端收到后回复SYN+ACK后，服务端进入SYN_RCVD状态，这个时候的socket会放入半连接队列
* 半连接队列的大小由/proc/sys/net/ipv4/tcp_max_syn_backlog控制



**全连接队列**

* 服务端收到客户端的ACK后，三次握手完成，此时socket会从半连接队列移出全连接队列
* 全连接队列大小通过/proc/sys/net/core/somaxconn



**socket函数对应的三次握手**

* 客户端发起connect：客户端向服务端发送SYN报文，客户端进入SYN_SEND状态

* accept阻塞：响应客户端，发送SYN+ACK报文给客户端，服务端进入SYN_RCVD状态

* connect函数返回：收到SYN+ACK报文段，客户端向服务端发送ACK报文，客户端进入ESTABLISHED状态

* accept函数返回：服务端收到ACK报文，服务端进入ESTABLISHED状态



**socket函数对应的四次握手**

* 客户端发起close，表示客户端已经没有数据需要发送，此时回向服务端发送FIN报文，进入FIN-WAIT-1状态
* 服务端收到FIN报文会插入一个EOF到缓冲区中待读取数据的末尾，服务端会发送ACK报文，进入CLOSE-WAIT状态
* 处理完数据以后当读取到EOF，服务端也会调用close关闭连接，向客户端发送FIN报文后进入LAST-ACK状态
* 客户端收到FIN报文后，会发送ACK报文，进入TW状态
* 服务端收到ACK报文最后进入CLOSE状态
* 客户端如果在2MSL未收到FIN报文则进入CLOSE状态





#### TCP三次握手异常分析



**TCP第一次握手SYN报文丢失，会发生什么？**

会进行重传。在Linux中，第一次握手超时重传次数由/proc/sys/net/ipv4/**tcp_syn_retries**决定，默认为5次，采用指数退避算法，每次RTO都会翻倍（1、2、4、8、16）



**TCP第二次握手SYN+ACK报文丢失，会发生什么？**

客户端会重传syn报文，重传次数由**tcp_syn_retries**决定，服务端会重传synack报文，重传次数由**tcp_synack_retries**决定



**TCP第三次握手ACK报文丢失，会发生什么？**

首先客户端会进入ESTABLISHED状态，服务端会进入SYN_RECV状态。服务端由于收不到ACK报文，会重传synack报文，重传次数由**tcp_synack_retries**决定，如果仍未反应则会进入CLOSED状态。客户端分两种情况

* 一种是建立后发送数据，TCP建立连接后，重传次数由tcp_retries2决定，默认为15次，如果仍未反应，那么会断开连接。
* 另一种情况是建立后不发送数据，此时会由保活机制决定，保活计时器会在某段时间内如果没有任何活动连接则会启动计时器，每隔一个时间间隔会检测，到达一定检测次数仍未反应则关闭连接。Linux可以设置相关的参数，tcp_keepalive_time表示保活时间，默认为2小时，tcp_keepalive_intvl表示检测间隔默认为75s，tcp_keepalive_probes表示检测次数，默认为9次。









### 可靠传输





#### 超时重传

* 设定一个超计时器，再发送完包后，如果超过指定时间未收到ACK确认报文，就重发该数据包
* 判定需要超时重传
  * 数据包丢失
  * 确认应答丢失



#### RTO

**RTT与RTO**

* RTT：包的往返时间
* RTO：超时重传时间





**RTO的设置**

* 如果设置过大，那么重传时间会变慢，会使网络的空闲时间增大，减低传输速率
* 如果设置过小，那么重传过快，会引起很多不必要的报文段重传，导致网络负荷增大
* 根据以上两种情况，RTO的设置应该是略大于RTT的。TCP采用一种自适应的做法，RTT表示为两个报文的往返时间，RTTs表示加权平均往返时间，每次取到一个新的RTT样本，就将旧的RTTs和RTT进行计算



**重传二义性**

发送方发送报文段后，设定的重传时间到了，还没收到确认，于是重传报文，经过一段时间后，收到了确认报文段，这种情况下，发送方并不不能判断此确认报文段是先发送的报文段的确认还是重传的报文段确认，由于RTO的计算是基于RTT样本的，所以错误的判断可能会导致RTO的计算出现偏差





**Karn算法**

* 报文段重传就不采用往返时间的样本，这样得到的RTTs和RTO就会比较准确
* 重传超时时，将重传时间设定为原来的2倍，当不再发生重传则按照原来的方法计算RTO



**带时间戳选项字段的RTT测量**

* 由于TCP有存在累积确认的情况，比如在传输大量数据时，TCP通常采用发送两个报文段返回一个ACK的方法，而并非每一个报文都返回一个ACK，这个时候可以选择使用时间戳字段



#### 快速重传

场景：连续收到3个相同的ACK情况下，会在重传定时器超时前，重传丢失报文。解决了超时时间的问题

缺陷：TCP无法确定是需要重传之前的一个包，还是重传所有，SACK机制解决了这个问题



#### SACK

在TCP头部的选项字段增加一个SACK，它缓存了接收以及未接收的包的信息，发送方根据SACK只发送重传的数据



#### DACK

为了解决接收方接收到了数据，但是发送的ACK报文丢失，导致接收方不必要的重传，接收方利用DACK机制可以告知接收方收到了重复的数据



#### 滑动窗口

* 停止等待协议

概念：每发送完一个分组就停止发送，等待对方确认。在收到确认后再发送下一个分组。这种可靠传输协议成为自动重传请求（ARQ）

评价：简单但是信道利用率低



* 连续ARQ协议

![](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fblog.chinaunix.net%2Fattachment%2F201402%2F17%2F26275986_1392626885IL2q.png&refer=http%3A%2F%2Fblog.chinaunix.net&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1613394039&t=50bdb1cb0a575020584d5be5d94f82aa)

评价：提高了信道的利用率，操作系统就是通过从缓冲区开辟滑动窗口实现的



**发送窗口**

滑动窗口以字节为单位，发送方维持发送窗口。从前往后的区间分别代表“已发送并且收到确认”、“已发送但未收到确认”、“未发送但是在发送窗口大小范围内”、“未在发送窗口范围内不允许发送”



**接收窗口**

与发送窗口类似，接收方维持接收窗口，从前往后区间分别代表“已接收并确认待应用”、“未收到但是可以接收的数据”、“未收到并不可以接受的数据”



**累计确认**

接收方一般采用累计确认机制，即不必对每一个收到的分组逐个发出确认，而是对按序到达的最后一个分组发送确认。这种方式的优点是容易实现，即使确认丢失也不必重传。缺点是不能向发送方正确反映已经正确收到的所有分组的信息。





### 流量控制

#### 概念

目的：让发送方的发送速率不要太快，要让接收方及时接收

实现：采用滑动窗口机制来进行流量控制



#### 零窗口

**概念**

接收方接收到了数据同时上层应用来不及处理数据，使得发送的窗口大小变小，当接收窗口为0时，会发送零窗口报文，此时不允许发送方再发送数据，这种情况即为零窗口。



**危害**

发送零窗口后不久，如果接收方的接收缓存又有了空间，这个时候接收方向发送方发送非零窗口报文丢失，导致接收方和发送方相互等待造成死锁



**解决方案**

TCP为每一个连接设置持续计时器。只要接收的一方收到了零窗口报文就启动持续计时器。若计时器到期就发送一个零窗口探测报文，而对方只要在确认这个零窗口探测报文给出自己的窗口值即可，若窗口值为0则重置计时器。这样就打破了死锁



#### 糊涂窗口综合症

**概念**

发送端的应用程序产生数据很慢或者接收端的应用数据处理接收缓冲区数据很慢，或者两者兼有之，这样会使得应用进程之间传送的报文段很小。比如接收方处理数据过慢，每次只能腾出几个字节并告诉发送方现在有几个字节的窗口。

**危害**

TCP+IP头部一共40个字节，如果每次只传输几个字节的数据，显然会造成巨大的开销，也会给网络增加不少负担。



**解决方案**

* 发送端解决办法（不能发送小的报文）：
  * 采用Nagle算法
* 接收端解决办法（不能通告小的窗口值）：
  * 采用Clark算法：只要有数据到达就发送确认，同时宣布窗口大小为0，直到缓存空间已经能放入具有最大长度报文段，或者缓存空间的一半已经空了
  * 延迟确认：当一个报文段到达时，不立即确认，接收端在确认收到报文段之前一直等待，直到缓存具有足够的空间为止





**Nagle算法**

Nagle算法规定，只有满足以下两个条件中的一条才能发送数据：

* 要等到窗口大小>=MSS或者数据大小>=MSS
* 收到之前发送数据的ack回包





**延时ACK**

* TCP并不对每个报文返回一个ACK，而是利用TCP累计ACK机制。累计确认允许TCP延迟一段时间发送ACK，以便将ACK和相同方向上需要传输的数据结合发送
* 应用场景：这个捎带传输机制适用于批量数据传输
* 评价：显然TCP不能任意时长地延迟ACK，否则对方会误认为数据丢失而出现不必要的重传，采用延时ACK的方法会减少ACK传输数目，从而减少网络负担





**延时ACK和Nagle算法**

* 延时ACK和Nagle算法都是为了减少网络负担所采取的措施，延时ACK是减少了ACK报文的数量，Nagle算法是减少了小包的发送，但是将这两种方式结合起来可能会造成某种程度的死锁
* 假设客户端采用的延时ACK是收到两个数据包才能返回一个ACK，而服务端使用了Nagle算法，直到收到ACK前都不能发送新数据，这样会导致死锁
* 幸运的是，死锁会在延时计时器超时后客户端发送ACK给服务端从而解除死锁，死锁期间会导致整个传输连接处于空闲状态





### 拥塞控制



#### 概念

* 原因：在某段时间内，网络所需资源超过该资源所能提供的可用部分，网络性能将会变坏

* 网络资源：计算机网络的链路带宽、交换结点的缓存和处理机

* 拥塞条件：∑对资源的需求>可用资源

* 拥塞控制：为防止过多的数据注入网络中，路由器无法处理高速到达的流量而被迫丢弃数据信息（拥塞的现象），这样可以使网络中的路由器或链路不致过载导致网络瘫痪

拥塞控制与流量控制区别

拥塞控制是一个全局性的过程，流量控制是一个端到端的问题，某些拥塞控制算法由于也是由接收方告诉发送方网络出现了拥塞，必须放慢发送速率

![](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fcaopeng.blog.chinaunix.net%2Fattachment%2F201303%2F13%2F24907956_1363163240YFVF.jpg&refer=http%3A%2F%2Fcaopeng.blog.chinaunix.net&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1613463811&t=79f725d68b18a08387e9cc9f48a2603b)





#### 慢启动

将cwnd设置为1个最大报文段MSS的数值，没收到一个新的报文的确认，cwnd增加1个MSS数值，这样拥塞窗口cwnd的值就随着传输轮次呈指数级增长。慢开始指它的起点低，用这样的方法来逐步增加拥塞窗口的值比一开始将cwnd值设大来说更加地合理

为防止拥塞窗口cwnd的值增加过大引起网络拥塞，还需要设置一个慢开始门限ssthresh

* 当cwnd < ssthresh时使用慢开始算法

* 当cwnd = ssthresh时既可以使用拥塞避免算法又可以使用慢开始算法

* 当cwnd > ssthresh时使用拥塞避免算法





#### 拥塞避免

* 当cwnd >= ssthresh时进入拥塞避免

* 思路：让拥塞窗口cwnd缓慢地增大，每经过一个RTT就把发送方的拥塞窗口cwnd加1/cwnd，因此拥塞避免阶段有加法增大的特点





#### 拥塞发生

发生丢包时，

* 超时重传（重传计时器超时）
* 快速重传（收到三个（重复的）ACK）

![](https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=3431860378,2888609999&fm=11&gp=0.jpg)

**超时重传**

无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（重传定时器超时），就会使用Tahoe算法

* ssthresh = max(cwnd/2，2)，cwnd = 1（Tahoe算法）



**快重传**

让发送方尽早知道发生了个别报文段的丢失。发送方只要接收到一连三个重复确认，就知道接收方没收到报文段，应当立即重传，这样就不会出现超时，发送方也不会误认出现了网络拥塞。一般快恢复于快重传一起使用，这两种算法考虑到了网络不太拥塞的情况

* cwnd = cwnd/2，ssthresh=cwnd
* 进入快恢复





#### 快恢复

当发送端收到连续三个重复的确认时，由于发送方现在认为网络很可能没有发生拥塞，因此现在不执行慢开始算法，而是执行快恢复算法 FR (Fast Recovery) 算法：

1. cwnd = ssthresh + 3
2. 重传丢失的数据包
3. 如果再收到重复的ACK，cwnd增加1
4. 收到新的数据的ACK后，把cwnd设置为之前的ssthresh的值
5. 开始执行拥塞避免算法，使拥塞窗口缓慢地线性增大。



**乘法减小加法增加**（AIMD）

加法增大：拥塞避免阶段，拥塞窗口采用线性增长 

乘法减小：快重传或快恢复时，出现超时或3个重复确认，慢开始门限值ssthresh设置成拥塞窗口cwnd的一半，并大大减小cwnd的值





**NewReno**

* Reno算法的缺点：若网络中一次拥塞丢失多个报文，采用Reno算法如果收到了新的ACK报文，就会立即退出快重传状态进入拥塞避免状态（PACK），这样Reno算法会误认为发生了多次拥塞，导致了拥塞窗口和慢启动阈值减半，最终导致TCP发送速率变慢

* 是对快重传算法的改进：既然Reno算法误判了一次拥塞丢失多个报文和多次拥塞的情况，那么NewReno算法就将这两种情况做区分，当所有报文被应答，问题就解决了，这就是NewReno算法的思想
* 做法：记录上一个数据传输窗口的最大序列号，只有收到了不小于该序列号的应答时，才退出快重传状态



### 其它



#### 如何实现安全可靠的udp

可以采用消息重传来实现其可靠性，采用消息重传的时候有两种方式，一种是发送者发起，另一种是接收者发起。对于发送者发起的方式，一般情况下接收者会发送一个消息包的确认。发送者维护一个计时器并重传那些在某个确定的时间段里没有收到确认的消息包。
对于接收者发起的方式，通信双方的接收者负责错误检测。在这个方式里，序列号被用于检测消息包丢失。当检测到消息包丢失，接收者请求发送者重传消息包。

发起者发起：这一类型的协议容易引起发送者溢出，因为要确认每一个发送的消息包。这种溢出现象被称为发送者（或者 ACK）内爆。

接收者发起：采用这种方法，如果消息包没有到达任何一个接收者，发送者容易因 NACK 溢出。这会引起发送者的负载过高和过多的重传。这种现像被称为 NACK内爆。Ramakrishnan et al.在 1987 年提出可以使用定时器来限制消息包重传，从而避免 NACK 内爆。在
现实应用中这种方式使用得较多。



#### UDP 伪首部

伪首部一共12个字节

* 源IP地址：4字节

* 目的IP地址：4字节

* 0字段：2字节，全零

* UDP协议字段：2字节，值为17

* UDP长度字段：2字节

  

伪首部仅参与计算校验和而不参与传输，发送方或者接收方根据IP报文获取12字节的伪首部并临时添加到首部前面来按16位一组来进行二进制反码求和再将求和结果的反码填入（检查）校验和字段。

udp中一个包大小究竟为多大合适

局域网环境下，建议将UDP数据控制在1472字节以下（以太网数据帧要在46字节-1500字节，IP数据包首部长度20字节，UDP首部长度8字节）

Internet编程时，建议将UDP数据控制在548字节以下（鉴于Internet上的标准MTU值为576字节， 576-20-8）





#### tcp 选项有什么 

* 时间戳选项和防回绕序列号：允许发送方针对每一个接收到的ACK估算TCP连接的往返时间，该选项由10个字节组成（4+4+2+2）发送方首先会将32位的数值填充到时间戳值字段作为时间戳字段的一部分，然后接收方将时间戳字段值复制到时间戳回显重试字段，其它的2字节用来指明选项的长度和数值，发送方接收到确认包后可以用来计算往返时间RTT，以便设置超时重传时间，第二点是为了处理TCP序号超过2^32的情况，这又被称为防回绕序列号，由于序列号最多为2 ^32 使用高速网络，序列号可能会被重复使用，因此可以将序列号看作拓展的序列号，避免相同序号的报文段产生二义性
* 最大报文段长度（MSS）选项：占4个字节，告知接收方能收到的最大报文段，最大段大小只记录TCP数据报的字节数，为什么需要这个字段？原因是，原因是如果MSS长度小，那么网络的利用率会下降，如果MSS长度较大，那么IP层传输就可能要分解成多个短报文，这样传输出错还要重传，开销增大，因此MSS应该尽可能大一些，只要在IP层传输不需要分片即可，不做设置一般默认为536字节（任何主机至少能够处理576字节的IPv4数据报，减去IPv4头部和TCP头部，则TCP发送的数据部分为576-20-20=536字节），最大段大小并不是通信双方协商的结果，只是一方表明不希望接收到比最大段还大的TCP报文段
* 窗口扩大选项：占3个字节，S为比例因子（0-14），新的窗口值等于16+S，因此窗口值最大可达2^30字节，该选项只能出现于SYN报文段，当连接建立后比例因子与方向绑定



#### tcp 异常处理，什么时候有RST

1. 连接未断开提前关闭服务器
2. 请求超时
3. 服务器端口未打开



#### 什么时候有PSH URG

* PSH：有时在一端的应用程序在键入一个命令后希望能立即接收对方的响应，这种情况下TCP就可以使用推送操作，而不用等到整个缓存都填满了后在向上交付
* URG：表明该报文段中有紧急数据，应尽快传送，例如已经发送了很长的程序要在远地主机运行，但后来发现了一些问题，需要取消该程序的运行，因此用户从键盘发出中断命令命令，如果不使用紧急数据，那么这两者字符将存储在接收TCP的缓存末尾，只有当处理完所有数据这两个字符才能被交付接收方的应用进程，这样做就浪费了许多时间





#### 常见的拥塞控制算法

* Tahoe
* reno算法，即上述拥塞控制所使用的算法
* newReno

* BBR算法，由Google研发





#### BBR算法 

怎么快，怎么实现，和之前的有什么区别 

* BBR在有一定的丢包率的网络链路上充分利用了带宽，因为不容易区分拥塞控制丢包和错误丢包，BBR就干脆不考虑将丢包作为拥塞的信号
* BBR不使用“加性增，乘性减”来维护窗口大小，而是分别估计极大带宽和极小延迟把它们的乘积作为发送窗口的大小
* BBR是基于发送端的延迟和带宽评估的算法

[BBR算法](https://www.zhihu.com/question/53559433)





#### TCP 粘包、拆包？解决办法

* 要发送的数据大于发送缓冲区剩余空间大小，将会发生拆包
* 要发送的数据大于MSS，TCP传输前会进行拆包

* 要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区数据一次发送出去，会发生粘包（Naggle算法）
* 接收数据端的应用层没有及时读取接收缓冲区的数据，将发生粘包

TCP是面向字节流的，无法理解上层的业务数据，所以底层无法保证数据包不被拆分和重组，这个问题只能由上层应用协议解决

关键在于如何区分数据包的边界。常用的方法有

1. 消息定长：发送端将每个数据包封装为固定长度
2. 设置消息边界：服务端从网络流中按消息边界分离出消息内容，在包尾增加回车换行进行分割
3. 将消息分为消息头和消息体：设置在消息前加入一个固定的结构体成为消息头，消息头中包含消息长度字段



#### TCP 封包和拆包 

封包就是给一段数据加上包头，这样一来数据包就分为包头和包体两部分内容了。包头其实上是个大
小固定的结构体，其中有个结构体成员变量表示包体的长度，这是个很重要的变量，其他的结构体成员可
根据需要自己定义。根据包头长度固定以及包头中含有包体长度的变量就能正确的拆分出一个完整的数据
包。
拆包就是根据包头以及包头中表示包长的结构体的变量，去掉包头的过程。

大概过程描述如下:
A,为每一个连接动态分配一个缓冲区,同时把此缓冲区和 SOCKET 关联,常用的是通过结构体关联.
B,当接收到数据时首先把此段数据存放在缓冲区中.
C,判断缓存区中的数据长度是否够一个包头的长度,如不够,则不进行拆包操作.
D,根据包头数据解析出里面代表包体长度的变量.
E,判断缓存区中除包头外的数据长度是否够一个包体的长度,如不够,则不进行拆包操作.
F,取出整个数据包.这里的"取"的意思是不光从缓冲区中拷贝出数据包,而且要把此数据包从缓存区
中删除掉.删除的办法就是把此包后面的数据移动到缓冲区的起始地址.





## 应用层

为每一类问题规定一种协议，许多协议都是基于BS方式



### DNS

解析：用于将主机IP转化为域名的分布式服务器

为什么机器在处理IP数据报要使用IP地址而不是域名：IP地址长度固定为32位而域名长度并不是固定的，机器处理起来会比较困难

为什么整个互联网不只使用一个域名服务器：会因为负荷而无法正常工作。域名解析系统被设计成为一个联机分布式的数据库系统，并且采用BS方式，DNS使大量名字都在本地解析，仅少量解析需要在互联网上通信。因此DNS的效率高。由于DNS使分布式系统，即使单个计算机出现故障，也不会妨碍整个DNS系统正常运行，而且分布式就近查询也解决了远距离的延迟问题



#### 域名服务器

根：根域名服务器是最高层次的域名服务器。

顶级域名：即TLD服务器，负责管理在该顶级域名服务器注册的所有二级域名。

权限域名：负责一个区的域名服务器

本地域名：当一台主机发出DNS查询请求，这个请求报文就会发送给本地域名服务器

#### 域名解析过程

为了提高域名的查找速度，设置了DNS多级缓存，距离由近到远有浏览器缓存、本地缓存、路由器缓存、IPS服务器缓存、DNS服务器缓存

1. 输入域名，主机先检查浏览器是否有该网站的映射关系，如果有则直接调用，否则执行2
2. 先检查本地hosts文件是否有该网站的映射关系，如果有则直接调用，否则执行3
3. 查找本地DNS解析器缓存，如果有则调用，否则执行3
4. TCP/IP参数设置的DNS服务器，也叫做本地DNS服务器，服务器收到查询时，如果查询的域名包含在本地配置的区域资源中，则将解析结果返回给客户，完成域名解析，此解析具有权威性，如果没有结果则执行4
5. 本地服务器向根服务器进行查询（例如www.baidu.com，本地服务器回向根服务器查询，根服务器收到.com域名会判断是谁来授权管理，并将结果返回给本地服务器，然后查找.com域名管理器的下一级DNS服务器baidu.com，当本地服务器收到这个地址后，就会查找baidu.com的域服务器，重复上述查找，知道找到www.baidu.com的主机）（迭代查询），缓存结果，最后将结果告知浏览器



递归查询：一般发生在主机与本地域名，当本地域名查找不到该域名对应的IP地址，本地域名就会以请求的方式，向根域名发送请求报文

迭代查询：一般发生在本地域名与根域名，根域名收到本地域名的请求报文时，根域名并不会帮助本地域名查找，而是返回能查找到该域名的服务器，然后本地域名再对指定服务器再一次进行请求





#### DNS欺骗

定义： DNS欺骗就是攻击者冒充域名服务器的一种欺骗行为。 

原理：如果可以冒充域名服务器，然后把查询的IP地址设为攻击者的IP地址，这样的话，用户上网就只能看到攻击者的主页，而不是用户想要取得的网站的主页了，这就是DNS欺骗的基本原理。DNS欺骗其实并不是真的“黑掉”了对方的网站，而是冒名顶替、招摇撞骗罢了。

方法：通过hosts文件修改、DNS劫持





### HTTP1.x



#### URI和URL

![](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic2.zhimg.com%2Fv2-274d4451e88937419e5abeb7acdb8425_b.png&refer=http%3A%2F%2Fpic2.zhimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1613550757&t=25f4d4b5ddbc1b2aca1907009a457d45)

URI：统一资源标识符

URL：统一资源定位符，格式（<协议>://<主机名>:<端口>/<路径>）

URN：统一资源名称



#### 报文格式

请求报文

![](https://raw.githubusercontent.com/MichaelDeSteven/CS-Note/master/%E7%BD%91%E7%BB%9C/pic/http请求.png)

* 请求行：请求方法 + URL + HTTP版本
* 请求头：由多个键值对组成，如User-Agent：产生请求的浏览器类型
* 请求体：使用POST请求数据在请求体上



响应报文

![](https://raw.githubusercontent.com/MichaelDeSteven/CS-Note/master/%E7%BD%91%E7%BB%9C/pic/http响应.png)

* 状态行：HTTP版本 + 状态码 + 解释状态码的短语
* 响应头：用于描述服务器的基本信息，以及数据描述，用于通知客户端如何处理等一下回送的数据
* 响应体：响应消息体，若响应文本类型的是HTML，则返回数据为HTML代码



#### HTTP方法

* GET：最常见的请求方式，使用GET方法时，将请求参数和对应的值放在URL后面，用？来代表URL的结尾和请求参数的开始
* POST：POST方法允许客户端给服务器提供较多的信息，POST请求参数将会封装在请求体上
* HEAD：HEAD就像GET方法，只不过服务端接收到HEAD请求后只返回响应头，而不返回响应内容。当我们查看某个页面状态时，HEAD非常高效
* PUT：上传文件，存在安全性问题
* DELETE：与PUT相反
* OPTIONS：查询指定URL支持的方法
* CONNECT：要求在与代理服务器通信时建立隧道





#### HTTP首部

有4种类型的首部字段：通用首部字段、请求首部字段、响应首部字段和实体首部字段



**通用首部字段**

| 首部字段名        | 说明                       |
| ----------------- | -------------------------- |
| Date              | 创建报文的日期时间         |
| Transfer-Encoding | 指定报文主体的传输编码方式 |
| Warning           | 错误通知                   |
| Cache-Control     | 控制缓存的行为             |
| Connection        | 逐跳首部、连接的管理       |



**请求首部字段**

| 首部字段名      | 说明                       |
| --------------- | -------------------------- |
| User-Agent      | 产生请求的浏览器类型       |
| Accept          | 客户端可识别的内容类型列表 |
| Accept-Charset  | 优先的字符集               |
| Accept-Encoding | 优先的内容编码             |
| Accept-Language | 优先的语言（自然语言）     |
| Host            | 请求资源所在服务器         |
| Range           | 字节范围内请求             |



**响应首部字段**

| 首部字段名称       | 说明                         |
| ------------------ | ---------------------------- |
| Location           | 令客户端重定向至指定 URI     |
| Accept-Ranges      | 是否接受字节范围请求         |
| Proxy-Authenticate | 代理服务器对客户端的认证信息 |



**实体首部字段**

| 首部字段名称     | 说明               |
| ---------------- | ------------------ |
| Content-Length   | 实体主体的大小     |
| Content-Type     | 实体主体的媒体类型 |
| Content-Language | 实体主体的自然语言 |



#### HTPP状态码

| 状态码 | 类别             | 含义                       |
| ------ | ---------------- | -------------------------- |
| 1xx    | 信息性状态码     | 接受的请求正在处理         |
| 2xx    | 成功状态码       | 请求正确处理完毕           |
| 3xx    | 重定向状态码     | 需要进行附加操作来完成请求 |
| 4xx    | 客户端错误状态码 | 服务器无法处理请求         |
| 5xx    | 服务器错误状态码 | 服务器处理请求错误         |

**1xx 信息**

* 100 Continue：服务器收到部分请求，等待客户端继续发送

**2xx 成功**

* 200 OK：客户端请求成功
* 204 No Content ：请求已经成功处理，但是返回的响应报文不包含实体的主体部分。
* 206 Partial Content ：表示客户端进行了范围请求，响应报文包含由 Content-Range 指定范围的实体内容。

**3xx 重定向**

* 301Moved Permanently 永久重定向
* 302 Temporarily Moved 临时重定向
* 304 Not Modified请求报文包含一些条件，如If-Match，If-None-Match，If-Range，如果满足条件则返回304

**4xx 客户端错误**

* 400 Bad Request ：请求报文中存在语法错误
* 401 Unauthorized ：该状态码表示用户未经授权
* 403 Forbidden ：请求被拒绝
* 404 Not Found：请求资源不存在

**5xx 服务端错误**

* 500 Internal Server Error ：服务器正在执行请求时发生错误
* 503 Service Unavailable ：服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。



**http1.0和http1.1区别**

1. 缓存处理：HTTP1.1引入了更多的缓存控制策略，新增了缓存处理指令max-age
2. 带宽优化与网络连接的使用：HTTP1.0中存在一些浪费带宽的现象，比如客户端只需要对象的一部分，而服务端却将整个对象传输过来，HTTP1.1，在请求头引入range，它允许只请求资源的某个部分，即返回码为206，这样方便开发者能更好地利用带宽
3. Host头处理：在请求头和请求体增加了Host头域，支持虚拟主机连接
4. 长连接：HTTP1.1支持长连接（设置Connection：keep-alive）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTPP请求和响应，减少了建立和关闭的消耗和延迟

[HTTP](https://blog.nowcoder.net/n/51b5ec0df6934e24af490234b7724ef4#-----)

**连接管理**

HTTP1.1中指定**Connection: keep-alive**，即可HTTP1.1默认长连接，长连接让传输效率大大提升，但是如果要在连接上发多个HTTP报文，多个报文会累积到队列里依次处理，只要队头的请求被阻塞了，后续的HTTP发送就会收到影响，这就是队头阻塞

队头阻塞很大的原因是因为TCP是面向流传输的，HTTP无法确定消息边界，因此也就无法区分每一个请求。接收方通过Content-Length来判断报文是否接收完毕，若接收完毕则可以直接请求解析，这样就解决了队头阻塞问题





#### 优化



**长连接**

从底层的传输层出发，将短连接改为长连接



**避免请求**

针对重复性的http请求，每次得到结果都一样，那么就将数据缓存到本地，下次就可以直接取缓存，避免了http请求



缓存过期问题：使用缓存头部问题进行解决



**减少重定向次数**

* 重定向：服务器上的资源由于迁移、维护等原因从url1迁移到url2，而客户端不知情，继续请求url1，这个时候需要通过302响应码和Location头部告诉客户端需要再发送url2请求以获得服务器的资源
* 代理服务器：由代理服务器完成重定向，就能减少HTTP请求次数



**合并请求**

* 将小的资源合并成大资源来减少HTTP请求次数，从而减少网络开销



**延时请求**

* 按需获取：当前不需要的资源就不获取



**压缩响应内容**

* Accept-Encoding：gzip、br





### HTTP2协议

#### HTTP/1.1缺陷

* 延迟难以下降

* 并发数量有限
* HTTP头部巨大且重复
* 不支持服务器推送
* 队头阻塞问题





#### 头部压缩

![](https://pic4.zhimg.com/80/v2-49eb023bd93fe6343f0c896b3de038cf_720w.jpg?source=1940ef5c)

HTTP的响应和请求都是由状态行、请求\响应头部，消息主体三部分组成，但是随着请求次数增大，而User-Agent这类基本不变的头部信息，每次要传输这种信息是一种浪费。为了压缩首部，采用了静态表、动态表以及哈夫曼编码



**静态编码**

HTTP/2框架维护一张静态表，记录常用的请求方法和头部名称



**动态编码**

对同一个连接上，重复传输完全相同的HTTP头部生成对应的index，以后传输都使用index来代替头部



**哈夫曼编码**

统计所有字符的出现频率，利用哈夫曼树的特点对字符进行编码，双方用来代替，大大减小了传输的数据量



#### 二进制帧

将响应报文分成一个个二进制帧

![](https://pic2.zhimg.com/80/906e22193e61cd561325d93aae0f1e07_720w.jpg?source=1940ef5c)

在二进制分帧层，HTTP2协议将所有传输的数据分割成更小的消息和帧，并对它们采用二进制格式编码，其中首部信息会被封装在HEADER帧而相应的Request Body则封装到DATA帧，采用了单连接多资源的方式，减少了服务端的压力，也使得连接的吞吐量增大





#### 并发请求

HTTP/1.1队头阻塞问题：HTTP1.1的同一个连接中，只有当一个事务处理完后，才能处理下一个事务。也就是在发送请求等待响应时，会被阻塞，后续的请求无法发出，这是HTTP层面的对头阻塞



**Stream、Frame、Message**

HTTP消息由多个帧构成，而一条Stream包含了包含了多个帧，HTTP/2连接中，多个Stream复用同一条连接，可以乱序发出，解决了HTTP/1.1队头阻塞问题





#### 服务器推送



![](https://pic1.zhimg.com/80/v2-9f97f2c92ddc3d56564642d6447d23d1_720w.jpg?source=1940ef5c)

页面加载时伴随着许多附加资源的载入，HTTP2采用了服务器推送机制，允许在浏览器和服务器连接后，服务器主动将一些资源推送给浏览器并缓存起来，若浏览器想要访问已缓存的资源，那么浏览器就可以直接读取缓存，充分利用了空闲的网络，改善页面加载时间。



实现方式

* 客户端发起的请求必须使用的是奇数号Stream，服务器主动推送，使用的是偶数号
* 服务器在推送资源的时候，会通过PUSH_PROMISE帧来传输HTTP头部，并通过帧中的Promised Stream ID字段告知客户端，接下来会是哪个偶数号的Stream中发送包体



### HTTP3协议



#### HTTP2缺陷

* HTTP/2虽然解决了HTTP/1.1在HTTP层面的队头阻塞问题，但是HTTP是基于TCP协议的，因此并解决不了TCP层面的队头阻塞问题
  * TCP队头阻塞：TCP是面向字节流的协议，因此TCP层必须保证收到的字节数据是完整且连续的。这样内核才会将缓冲区的数据返回给HTTP应用。那么当前1字节数据没有到达时，后面收到的字节数据只能存放在内核缓冲区里面。这就是HTTP/2的队头阻塞问题
* TCP与TLS1.2的握手延迟。发起HTTP请求需要经过三次握手和SSL四次握手，因此需要经过3个RTT才能开始传输数据
* 网络迁移需要重新连接：一个TCP连接由四元组组成，这意味着如果IP地址或者端口变动需要重新连接



#### QUIC协议

QUIC 特点，缺点

QUIC 是 Quick UDP Internet Connections 的缩写，谷歌发明的新传输协议。

与TCP相比，QUIC可以减少延迟。未被普及





![](https://pic1.zhimg.com/v2-fa0fa846a1c792845356f590a25f5574_b.png)

**解决什么问题**

* 依赖操作系统内核的协议僵化：TCP是由操作系统内核实现的，应用程序只能使用而不能修改，如果需要对TCP迭代更新，就需要升级操作系统，由于操作系统升级涉及到了底层软件和运行库的更新，因此更新也较为保守和缓慢，这导致了TCP即使拥有较好的特性更新，但是很难快速推广
* 建立连接的握手延迟大：HTTP需要使用TCP进行传输，同时HTTPS和HTTP2还需要TLS协议进行安全传输，这就导致了两次握手延迟，对于很多短连接的场景，这样的握手延迟影响大，且无法消除
* 队头阻塞：队头阻塞的根本原因是TCP是面向流传输的，使用序列号来标识数据的顺序，数据必须按照顺序处理，如果前面的数据丢失，后面的数据就算到达也不会被处理
* 所以QUIC协议选择了UDP，首先UDP是面向无连接的，不需要三次握手，优化了连接握手的延迟，同时可以在应用层面实现传输的可靠性，避开了操作系统的限制，提高了灵活性



**QUIC特性**



* 连接建立延迟低：TCP连接需要经过3次握手，QUIC实现了0RTT握手，先前与服务器建立连接的客户端缓存了大部分连接参数，因此在大部分情况下，不需要建立连接就能够传输数据
  * QUIC内置TLS1.3，同时建立连接时会在自己的帧携带TLS的记录，因此仅需要1RTT就可以完成建立连接和密钥协商，甚至在第二次连接时，由于本地缓存了密钥协商的相关信息，应用数据包可以携带连接信息和TLS信息一起发送，达到0RTT的效果
* 连接迁移：QUIC采用连接ID的方式来标识通信双方，即使IP地址或者端口发生变化，只要仍然保留连接ID、TLS信息就可以复用本次连接，达到连接迁移的效果
* 没有队头阻塞的多路复用：QUIC的多路复用与HTTP2类似，都是在一个连接上有多个请求，不同的是，QUIC**各个stream互不影响**，避免了对头阻塞的问题，同时UDP是基于面向报文的，所以QUIC的传输单元为报文，数据传输单元不会跨包，也避免了对头阻塞问题
* 拥塞控制的实现，TCP的拥塞控制主要有慢启动、拥塞避免、快重传、快恢复，QUIC使用了Cubic拥塞控制算法，同时还对拥塞控制进行了一些改进
  * 可插拔：主要体现三个方面，应用程序可以实现不同的拥塞控制算法，不用依赖操作系统，单个应用程序对不同连接可以采用不同的拥塞控制，应用程序不需要停机和升级就能对拥塞控制进行更换
  * Package Number和Stream Offset：TCP采用的是序列号和确认号来确认消息的有序到达，QUIC使用了严格递增Package Number代替了序列号，严格递增避免了重传二义性，但是重传时仍然需要确认包的顺序，所以有了Stream Offset保证了数据的有序性
  * SACK选项：TCP头部最大为60字节，固定长度为20字节，时间戳字段占10字节，而SACK选项占8字节，因此最多支持3个区间选择重传，但是QUIC提供了256的Ack Block，丢包率较高的情况下，能够快速的恢复网络
  * 延迟ACK：TCP的时间戳字段存在一个问题，接收方只是回显了发送方的时间戳，却没有考虑从回显到发送Ack的时间，导致RTT的误差，QUIC在计算RTT时考虑了延迟ACK，使得RTT更加准确



[QUIC协议](https://mp.weixin.qq.com/s/vpz6bp3PT1IDzZervyOfqwhttps://mp.weixin.qq.com/s/vpz6bp3PT1IDzZervyOfqw)



### HTTPS协议



#### HTTP缺点

* 使用明文进行通信，内容可能会被窃听（报文窃听）
* 不验证通信方的身份，通信方的身份有可能是伪造（身份伪造）
* 无法证明报文的完整性，报文有可能遭篡改（报文篡改）



#### HTTP与HTTPS

* 端口：HTTPS为443，HTTP为80
* 安全：HTTP是超文本传输协议，信息是明文传输，HTTPS是具有安全性的传输协议
* HTTPS运行在SSL/TLS之上
* 费用：HTTP免费，HTTPS的CA证书需要一定费用



#### **密码学基础**
在正式讲解HTTPS协议之前，我们首先要知道一些密码学的知识。

明文： 明文指的是未被加密过的原始数据。

密文：明文被某种加密算法加密之后，会变成密文，从而确保原始数据的安全。密文也可以被解密，得到原始的明文。

密钥：密钥是一种参数，它是在明文转换为密文或将密文转换为明文的算法中输入的参数。密钥分为对称密钥与非对称密钥，分别应用在对称加密和非对称加密上。

对称加密：对称加密又叫做私钥加密，即信息的发送方和接收方使用同一个密钥去加密和解密数据。对称加密的特点是算法公开、加密和解密速度快，适合于对大数据量进行加密，常见的对称加密算法有DES、3DES、TDEA、Blowfish、RC5和IDEA。
其加密过程如下：明文 + 加密算法 + 私钥 => 密文
解密过程如下：密文 + 解密算法 + 私钥 => 明文
对称加密中用到的密钥叫做私钥，私钥表示个人私有的密钥，即该密钥不能被泄露。
其加密过程中的私钥与解密过程中用到的私钥是同一个密钥，这也是称加密之所以称之为“对称”的原因。由于对称加密的算法是公开的，所以一旦私钥被泄露，那么密文就很容易被破解，所以**对称加密的缺点是密钥安全管理困难**。

非对称加密：非对称加密也叫做公钥加密。非对称加密与对称加密相比，其安全性更好。对称加密的通信双方使用相同的密钥，如果一方的密钥遭泄露，那么整个通信就会被破解。而非对称加密使用一对密钥，即公钥和私钥，且二者成对出现。私钥被自己保存，不能对外泄露。公钥指的是公共的密钥，任何人都可以获得该密钥。用公钥或私钥中的任何一个进行加密，用另一个进行解密。
被公钥加密过的密文只能被私钥解密，过程如下：
明文 + 加密算法 + 公钥 => 密文， 密文 + 解密算法 + 私钥 => 明文
被私钥加密过的密文只能被公钥解密，过程如下：
明文 + 加密算法 + 私钥 => 密文， 密文 + 解密算法 + 公钥 => 明文
由于加密和解密使用了两个不同的密钥，这就是非对称加密“非对称”的原因。
**非对称加密的缺点是加密和解密花费时间长、速度慢**，只适合对少量数据进行加密。
在非对称加密中使用的主要算法有：RSA、Elgamal、Rabin、D-H、ECC（椭圆曲线加密算法）等。



RSA算法步骤

* 选择两个超大素数p，q
* 计算n = pq，z = （p - 1）（q - 1）
* 选择一个数e，满足e小于n，且e与z公因数为1
* 求一个数d，满足edMODz=1
* 那么得到的公钥为（n，e），私钥为（n，d）
* 加密值$c=m^emodn$
* 解密值$m=c^dmodn$





#### 四次握手



HTTPS为了兼顾安全与效率，同时使用了对称加密和非对称加密。数据是被对称加密传输的，对称加密过程需要客户端的一个密钥，为了确保能把该密钥安全传输到服务器端，采用非对称加密对该密钥进行加密传输，总的来说，**对数据进行对称加密，对称加密所要使用的密钥通过非对称加密传输。**

HTTPS在传输的过程中会涉及到三个密钥：
服务器端的公钥和私钥，用来进行非对称加密
客户端生成的随机密钥，用来进行对称加密
一个HTTPS请求实际上包含了两次HTTP传输，可以细分为8步。

1. 客户端向服务器发起HTTPS请求，连接到服务器的443端口
2. 服务器有一个密钥对，即公钥和私钥，是用来进行非对称加密使用的，服务器保存则私钥，不将其泄露，公钥可以发给任何人
3. 服务器将公钥发给客户端
4. 客户端收到服务端的公钥后，会对其合法性检查，如果发现公钥有问题，HTTPS传输就无法继续。如果公钥合格，客户端就会生成随机值，这个随机值就是用来对称加密的密钥，即客户端密钥。然后客户端用服务器的公钥和自己生成的密钥进行非对称加密，客户端密钥就变成了密文，至此HTTPS的第一次请求接收
5. 客户端发起HTTPS中的第二个HTTP请求，将加密之后的客户端密钥发给服务器
6. 服务器收到客户端发来的密文之后，会用自己的私钥对其进行非对称解密，解密之后的明文就是客户端密钥，然后用客户端密钥堆数据进行对称加密，这样数据就变成了密文
7. 服务器将加密的数据发给客户端
8. 客户端收到服务器发来的密文，用自己的密钥对其进行对称加密，得到服务器的数据，这样就完成了HTTPS的响应



#### **CA与证书**

![](C:\Users\Steven\Desktop\CS-Note\网络\pic\数字证书.png)

数字证书就是由客户端和服务端双方都信任的**数字证书认证机构**颁发的，服务端的人员通过向机构提出证书申请，批准后会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，服务端会自己保留一份私钥，客户端向服务端请求时，服务端会将生成数字证书，这个证书内容分明文部分和密文部分，明文包含了**公钥内容，签发者信息，有效期**等，密文内容是对明文内容使用了**hash加密算法**，hash加密是**单向**的，也就是只能从明文转为密文而不能从密文转为明文，将该证书发送给客户端，客户端收到后，会对明文内容进行hash加密，然后与密文内容进行比较，不一致说明明文内容可能被篡改或者证书不是由CA签发，一致那么就签名合法，就**获得了服务端的公钥**







### DHCP协议

介绍：DHCP协议提供了即插即用连网机制，这种机制允许一台计算机加入新的网络和获取IP地址时不用手工。参与需要IP地址的主机在启动时就向DHCP服务器广播发送发现报文，这时该主机就成为DHCP客户。DHCP服务器收到该报文后，会从IP地址池中取一个地址分配给该计算机，DHCP服务器的回答报文叫做提供报文



### FTP协议

特点：FTP服务器进程分为两大部分

* 主进程：负责接收新的请求
* 从属进程：负责处理单个请求







**常用端口**

| 应用               | 应用协议 | 端口号 | 传输层协议 | 备注                       |
| ------------------ | -------- | ------ | ---------- | -------------------------- |
| 域名解析           | DNS      | 53     | UDP/TCP    | 长度超过512字节使用TCP协议 |
| 动态主机配置协议   | DHCP     | 67/68  | UDP        |                            |
| 文件传输协议       | FTP      | 20/21  | TCP        | 控制连接21，数据连接20     |
| 超文本传输协议     | HTTP     | 80     | TCP        |                            |
| 超文本传输加密协议 | HTTPS    | 443    | TCP        |                            |



### 相关问题



Http1.1 特点，缺点

* 默认支持长连接，支持流水线机制
* 增加了缓存控制
* 支持虚拟主机HTTP访问
* 允许字节范围内请求，返回码为206

http2.0 特点，缺点

* 多路复用，使用了二进制分帧层
* 头部压缩
* 支持服务器推送



http3.0 特点，缺点

http 和 https



可以伪造证书吗？中间人攻击能预防吗？



对称加密和非对称加密，RSA 具体说说



DES 和 AES 区别



HTTPS 证书是啥？加密内容？



HTTPS与HTTP的区别，HTTPS密钥交换过程，CA信任链



CA证书的验证过程



[https原理](https://www.javadoop.com/post/https)





http的请求过程，https的加密过程



https的连接过程



HTTP请求报文格式：首行、headers、空行、body

请求行、请求头、请求体、响应行、响应头、响应体都包括什么

HTTP状态码？304 Not Modifed/400系列讲一下



get和post的区别

含义：GET用于获取资源，POST用于传输实体主体

* 提交方式：GET提交，提交的数据会附在URL之后，以？分割URL和提交数据，多个参数则用&连接，如果是中文则直接把字符串用BASE64加密。POST提交把提交的数据放置在是HTTP请求体。

* GET提交的数据会在地址栏中显示出来，而POST提交，地址栏不会改变。

* 传输数据的大小：虽然HTTP协议没有对传输的数据大小进行限制。但是GET请求的长度受限于浏览器对URL长度的限制。因此对于GET提交时，传输数据就会受到URL长度的限制。POST由于不是通过URL传值，理论上数据不受限。

* 幂等性：所谓幂等性是指HTTP请求执行一次和执行多次的效果是一样的。GET方法是幂等的，POST方法不是幂等的。



HTTP如何实现缓存，怎样告诉浏览器这个可以被缓存以及缓存时间

* 缓存的目的，首先是为了减少服务端的压力，其次降低了用户请求资源的延迟

* HTTP使用Cache-Controll来控制缓存策略
* Cache-Controll有如下参数
  * no-store：表示禁止缓存
  * no-cache：缓存服务器必须向源服务器验证缓存资源的有效性，只有当缓存资源有效时才能使用该缓存对客户端请求进行响应
  * private：规定将资源作为私有资源，只能被用户单独使用
  * public：规定将资源作为公有资源，可以被多个用户共享使用
* 缓存过期机制
  * max-age：出现在请求报文并且缓存资源的缓存时间小于该指令的指定时间，那么就接受该缓存，出现在响应报文，则表示缓存资源在缓存服务器保存的时间
  * Expires：告知缓存服务器该缓存什么时候过期
* 缓存验证ETag



输入一个url发生了什么

1. DNS解析
2. TCP三次握手
3. 若为https则需要进行验证，否则为普通的http连接
4. 浏览器发送请求报文
5. 请求报文内容
6. 服务端发送响应报文
7. 响应报文内容
8. 浏览器收到数据渲染页面。若为短连接则关闭TCP连接即四次握手





HTTP的长连接和实现原理

长连接是干啥用的？举个场景说

长连接的连接数有限制吗？

* Connection：keepAlive用来告诉对方请求响应完后不要断开连接

keep_alive参数

* timeout：过期时间，如果过期时间间隔内没有请求则断开连接
* max：最大请求次数，如果为0则强制断开





为什么使用Session和Cookie

HTTP的请求响应是无状态的，也就是说服务器并不区分客户是谁，但是有时候是需要保存客户端的一些信息的，需要去记录客户端的连接状态、识别请求状态。为了解决这一类问题，就需要使用到Cookie和Session

Session和Cookie的区别以及如何解决分布式session问题

* Cookie：客户端访问某个地址，会将请求交给服务器进行处理，浏览器会将请求内容一并交给服务端处理。在处理的过程中，Cookie在服务端保存，生成Cookie对象时，需要确定具体的名称和到期时间，服务端再将Cookie发送给浏览器，浏览器接收到后，会将Cookie信息保存在本地，下次访问该网页进行请求时，会附带Cookie信息一并发送给服务端
* Session：Session在服务端生成，存储在服务器端，即存在内存中。一般来说一个SessionID就对应了一个用户



总结

1. Cookie存在客户端、Session存在服务端
2. 安全性要求较高使用Session，要求低使用Cookie
3. Cookie只能存储字符串，Session可以存储任何信息
4. 在存储相对持久的信息，应当考虑使用Cookie。在进行登录的验证或者信息的拦截，可以使用Session
5. Session由于存储在服务端，如果大量用户信息都用Session存储会让服务器压力增大，因此不宜将所有用户信息都用Session存储



SSL握手和工作流程，SSH/SSL的区别，详细说明

对称加密和非对称加密

DNS查询过程，递归查询和迭代查询的区别





跨域了解吗？

cookie支持跨域吗？

cookie可以包括哪些类型的数据？

csrf 预防，http/dns 劫持

dns欺骗怎么办？ arp欺骗？CSRF攻击？xss攻击？syn flood攻击？应对方法





断点续传原理





## 网络安全

### IP欺骗

通过IP地址伪装成某台主机，使得这台主机被另外的主机所信任





### TCP重置攻击

TCP重置攻击中，攻击者通过向通信双方中的一方或者双方发送伪造的数据，告诉他们立刻断开连接，从而使通信双方连接中断，它利用了TCP重置机制，当接收的报文段对于相关连接不正确，那么TCP会发送一个重置报文，从而导致TCP连接重建，攻击者通过伪造重置报文，防止连接被用来进一步交换信息，这种攻击只对于长连接有杀伤力，因为短连接，在完成信息交换就已经断开连接了



伪造报文的步骤

* 嗅探通信双方交换信息
* 截获一个ACK报文段，并读取其ack号
* 伪造一个TCP重置报文段（RST=1）将序列号置为ack号





### 中间人攻击

#### 原理

中间人攻击指的是攻击者与通信双方建立连接，使得通信双方都认为自己通过一个私密连接与对方直接对话，实际上整个对话都被攻击者完全控制



